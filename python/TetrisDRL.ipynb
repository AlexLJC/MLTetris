{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    This Notebook is for learning only\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    This Notebook is for learning only\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "import tetris as tetris\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameter \n",
    "\n",
    "num_episodes = 500\n",
    "num_exploration_episodes = 100\n",
    "max_len_episode = 1000\n",
    "batch_size = 40\n",
    "learning_rate = 0.005\n",
    "gamma = 0.95\n",
    "initial_epsilon = 1.0\n",
    "final_epsilon = 0.01\n",
    "\n",
    "eps_decay = 0.995\n",
    "eps_min = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 28 Total Steps 27\n",
      "EP0 EpisodeReward=17\n",
      "Score 14 Total Steps 14\n",
      "EP1 EpisodeReward=3\n",
      "Score 24 Total Steps 24\n",
      "EP2 EpisodeReward=13\n",
      "Score 17 Total Steps 17\n",
      "EP3 EpisodeReward=6\n",
      "Score 20 Total Steps 20\n",
      "EP4 EpisodeReward=9\n",
      "Score 16 Total Steps 16\n",
      "EP5 EpisodeReward=5\n",
      "Score 22 Total Steps 22\n",
      "EP6 EpisodeReward=11\n",
      "Score 17 Total Steps 17\n",
      "EP7 EpisodeReward=6\n",
      "Score 16 Total Steps 16\n",
      "EP8 EpisodeReward=5\n",
      "Score 24 Total Steps 24\n",
      "EP9 EpisodeReward=13\n",
      "Score 18 Total Steps 17\n",
      "EP10 EpisodeReward=7\n",
      "Score 14 Total Steps 14\n",
      "EP11 EpisodeReward=3\n",
      "Score 16 Total Steps 16\n",
      "EP12 EpisodeReward=5\n",
      "Score 21 Total Steps 21\n",
      "EP13 EpisodeReward=10\n",
      "Score 12 Total Steps 12\n",
      "EP14 EpisodeReward=1\n",
      "Score 18 Total Steps 18\n",
      "EP15 EpisodeReward=7\n",
      "Score 15 Total Steps 15\n",
      "EP16 EpisodeReward=4\n",
      "Score 16 Total Steps 16\n",
      "EP17 EpisodeReward=5\n",
      "Score 10 Total Steps 10\n",
      "EP18 EpisodeReward=-1\n",
      "Score 20 Total Steps 20\n",
      "EP19 EpisodeReward=9\n",
      "Score 21 Total Steps 21\n",
      "EP20 EpisodeReward=10\n",
      "Score 19 Total Steps 19\n",
      "EP21 EpisodeReward=8\n",
      "Score 22 Total Steps 22\n",
      "EP22 EpisodeReward=11\n",
      "Score 18 Total Steps 18\n",
      "EP23 EpisodeReward=7\n",
      "Score 16 Total Steps 16\n",
      "EP24 EpisodeReward=5\n",
      "Score 23 Total Steps 23\n",
      "EP25 EpisodeReward=12\n",
      "Score 22 Total Steps 22\n",
      "EP26 EpisodeReward=11\n",
      "Score 12 Total Steps 12\n",
      "EP27 EpisodeReward=1\n",
      "Score 21 Total Steps 21\n",
      "EP28 EpisodeReward=10\n",
      "Score 23 Total Steps 23\n",
      "EP29 EpisodeReward=12\n",
      "Score 21 Total Steps 21\n",
      "EP30 EpisodeReward=10\n",
      "Score 24 Total Steps 24\n",
      "EP31 EpisodeReward=13\n",
      "Score 23 Total Steps 23\n",
      "EP32 EpisodeReward=12\n",
      "Score 21 Total Steps 21\n",
      "EP33 EpisodeReward=10\n",
      "Score 17 Total Steps 17\n",
      "EP34 EpisodeReward=6\n",
      "Score 21 Total Steps 21\n",
      "EP35 EpisodeReward=10\n",
      "Score 24 Total Steps 24\n",
      "EP36 EpisodeReward=13\n",
      "Score 22 Total Steps 22\n",
      "EP37 EpisodeReward=11\n",
      "Score 23 Total Steps 23\n",
      "EP38 EpisodeReward=12\n",
      "Score 20 Total Steps 20\n",
      "EP39 EpisodeReward=9\n",
      "Score 25 Total Steps 25\n",
      "EP40 EpisodeReward=14\n",
      "Score 24 Total Steps 24\n",
      "EP41 EpisodeReward=13\n",
      "Score 19 Total Steps 19\n",
      "EP42 EpisodeReward=8\n",
      "Score 22 Total Steps 22\n",
      "EP43 EpisodeReward=11\n",
      "Score 22 Total Steps 22\n",
      "EP44 EpisodeReward=11\n",
      "Score 21 Total Steps 21\n",
      "EP45 EpisodeReward=10\n",
      "Score 21 Total Steps 21\n",
      "EP46 EpisodeReward=10\n",
      "Score 23 Total Steps 23\n",
      "EP47 EpisodeReward=12\n",
      "Score 21 Total Steps 21\n",
      "EP48 EpisodeReward=10\n",
      "Score 11 Total Steps 11\n",
      "EP49 EpisodeReward=0\n",
      "Score 23 Total Steps 23\n",
      "EP50 EpisodeReward=12\n",
      "Score 12 Total Steps 12\n",
      "EP51 EpisodeReward=1\n",
      "Score 21 Total Steps 21\n",
      "EP52 EpisodeReward=10\n",
      "Score 18 Total Steps 18\n",
      "EP53 EpisodeReward=7\n",
      "Score 26 Total Steps 26\n",
      "EP54 EpisodeReward=15\n",
      "Score 23 Total Steps 23\n",
      "EP55 EpisodeReward=12\n",
      "Score 19 Total Steps 19\n",
      "EP56 EpisodeReward=8\n",
      "Score 23 Total Steps 23\n",
      "EP57 EpisodeReward=12\n",
      "Score 25 Total Steps 25\n",
      "EP58 EpisodeReward=14\n",
      "Score 28 Total Steps 28\n",
      "EP59 EpisodeReward=17\n",
      "Score 12 Total Steps 12\n",
      "EP60 EpisodeReward=1\n",
      "Score 21 Total Steps 21\n",
      "EP61 EpisodeReward=10\n",
      "Score 24 Total Steps 24\n",
      "EP62 EpisodeReward=13\n",
      "Score 26 Total Steps 26\n",
      "EP63 EpisodeReward=15\n",
      "Score 21 Total Steps 21\n",
      "EP64 EpisodeReward=10\n",
      "Score 23 Total Steps 23\n",
      "EP65 EpisodeReward=12\n",
      "Score 25 Total Steps 25\n",
      "EP66 EpisodeReward=14\n",
      "Score 28 Total Steps 28\n",
      "EP67 EpisodeReward=17\n",
      "Score 22 Total Steps 22\n",
      "EP68 EpisodeReward=11\n",
      "Score 27 Total Steps 27\n",
      "EP69 EpisodeReward=16\n",
      "Score 28 Total Steps 28\n",
      "EP70 EpisodeReward=17\n",
      "Score 24 Total Steps 24\n",
      "EP71 EpisodeReward=13\n",
      "Score 23 Total Steps 23\n",
      "EP72 EpisodeReward=12\n",
      "Score 27 Total Steps 27\n",
      "EP73 EpisodeReward=16\n",
      "Score 25 Total Steps 25\n",
      "EP74 EpisodeReward=14\n",
      "Score 29 Total Steps 28\n",
      "EP75 EpisodeReward=18\n",
      "Score 18 Total Steps 18\n",
      "EP76 EpisodeReward=7\n",
      "Score 15 Total Steps 15\n",
      "EP77 EpisodeReward=4\n",
      "Score 10 Total Steps 10\n",
      "EP78 EpisodeReward=-1\n",
      "Score 14 Total Steps 14\n",
      "EP79 EpisodeReward=3\n",
      "Score 24 Total Steps 24\n",
      "EP80 EpisodeReward=13\n",
      "Score 12 Total Steps 12\n",
      "EP81 EpisodeReward=1\n",
      "Score 15 Total Steps 15\n",
      "EP82 EpisodeReward=4\n",
      "Score 18 Total Steps 18\n",
      "EP83 EpisodeReward=7\n",
      "Score 24 Total Steps 24\n",
      "EP84 EpisodeReward=13\n",
      "Score 23 Total Steps 23\n",
      "EP85 EpisodeReward=12\n",
      "Score 22 Total Steps 22\n",
      "EP86 EpisodeReward=11\n",
      "Score 25 Total Steps 25\n",
      "EP87 EpisodeReward=14\n",
      "Score 24 Total Steps 24\n",
      "EP88 EpisodeReward=13\n",
      "Score 24 Total Steps 24\n",
      "EP89 EpisodeReward=13\n",
      "Score 24 Total Steps 24\n",
      "EP90 EpisodeReward=13\n",
      "Score 22 Total Steps 22\n",
      "EP91 EpisodeReward=11\n",
      "Score 25 Total Steps 25\n",
      "EP92 EpisodeReward=14\n",
      "Score 22 Total Steps 22\n",
      "EP93 EpisodeReward=11\n",
      "Score 21 Total Steps 21\n",
      "EP94 EpisodeReward=10\n",
      "Score 28 Total Steps 28\n",
      "EP95 EpisodeReward=17\n",
      "Score 29 Total Steps 29\n",
      "EP96 EpisodeReward=18\n",
      "Score 20 Total Steps 20\n",
      "EP97 EpisodeReward=9\n",
      "Score 23 Total Steps 23\n",
      "EP98 EpisodeReward=12\n",
      "Score 24 Total Steps 23\n",
      "EP99 EpisodeReward=13\n",
      "Score 20 Total Steps 20\n",
      "EP100 EpisodeReward=9\n",
      "Score 21 Total Steps 21\n",
      "EP101 EpisodeReward=10\n",
      "Score 26 Total Steps 26\n",
      "EP102 EpisodeReward=15\n",
      "Score 23 Total Steps 23\n",
      "EP103 EpisodeReward=12\n",
      "Score 27 Total Steps 27\n",
      "EP104 EpisodeReward=16\n",
      "Score 23 Total Steps 23\n",
      "EP105 EpisodeReward=12\n",
      "Score 22 Total Steps 22\n",
      "EP106 EpisodeReward=11\n",
      "Score 21 Total Steps 21\n",
      "EP107 EpisodeReward=10\n",
      "Score 27 Total Steps 27\n",
      "EP108 EpisodeReward=16\n",
      "Score 17 Total Steps 17\n",
      "EP109 EpisodeReward=6\n",
      "Score 22 Total Steps 22\n",
      "EP110 EpisodeReward=11\n",
      "Score 29 Total Steps 29\n",
      "EP111 EpisodeReward=18\n",
      "Score 24 Total Steps 24\n",
      "EP112 EpisodeReward=13\n",
      "Score 22 Total Steps 22\n",
      "EP113 EpisodeReward=11\n",
      "Score 14 Total Steps 14\n",
      "EP114 EpisodeReward=3\n",
      "Score 20 Total Steps 20\n",
      "EP115 EpisodeReward=9\n",
      "Score 12 Total Steps 12\n",
      "EP116 EpisodeReward=1\n",
      "Score 10 Total Steps 10\n",
      "EP117 EpisodeReward=-1\n",
      "Score 20 Total Steps 20\n",
      "EP118 EpisodeReward=9\n",
      "Score 13 Total Steps 13\n",
      "EP119 EpisodeReward=2\n",
      "Score 24 Total Steps 24\n",
      "EP120 EpisodeReward=13\n",
      "Score 20 Total Steps 20\n",
      "EP121 EpisodeReward=9\n",
      "Score 22 Total Steps 22\n",
      "EP122 EpisodeReward=11\n",
      "Score 29 Total Steps 29\n",
      "EP123 EpisodeReward=18\n",
      "Score 26 Total Steps 26\n",
      "EP124 EpisodeReward=15\n",
      "Score 10 Total Steps 10\n",
      "EP125 EpisodeReward=-1\n",
      "Score 25 Total Steps 25\n",
      "EP126 EpisodeReward=14\n",
      "Score 25 Total Steps 25\n",
      "EP127 EpisodeReward=14\n",
      "Score 26 Total Steps 26\n",
      "EP128 EpisodeReward=15\n",
      "Score 13 Total Steps 13\n",
      "EP129 EpisodeReward=2\n",
      "Score 27 Total Steps 27\n",
      "EP130 EpisodeReward=16\n",
      "Score 26 Total Steps 26\n",
      "EP131 EpisodeReward=15\n",
      "Score 21 Total Steps 21\n",
      "EP132 EpisodeReward=10\n",
      "Score 30 Total Steps 30\n",
      "EP133 EpisodeReward=19\n",
      "Score 28 Total Steps 28\n",
      "EP134 EpisodeReward=17\n",
      "Score 23 Total Steps 23\n",
      "EP135 EpisodeReward=12\n",
      "Score 23 Total Steps 23\n",
      "EP136 EpisodeReward=12\n",
      "Score 18 Total Steps 18\n",
      "EP137 EpisodeReward=7\n",
      "Score 24 Total Steps 24\n",
      "EP138 EpisodeReward=13\n",
      "Score 24 Total Steps 24\n",
      "EP139 EpisodeReward=13\n",
      "Score 20 Total Steps 20\n",
      "EP140 EpisodeReward=9\n",
      "Score 28 Total Steps 28\n",
      "EP141 EpisodeReward=17\n",
      "Score 25 Total Steps 25\n",
      "EP142 EpisodeReward=14\n",
      "Score 28 Total Steps 28\n",
      "EP143 EpisodeReward=17\n",
      "Score 11 Total Steps 11\n",
      "EP144 EpisodeReward=0\n",
      "Score 9 Total Steps 9\n",
      "EP145 EpisodeReward=-2\n",
      "Score 27 Total Steps 27\n",
      "EP146 EpisodeReward=16\n",
      "Score 24 Total Steps 24\n",
      "EP147 EpisodeReward=13\n",
      "Score 22 Total Steps 22\n",
      "EP148 EpisodeReward=11\n",
      "Score 22 Total Steps 22\n",
      "EP149 EpisodeReward=11\n",
      "Score 22 Total Steps 22\n",
      "EP150 EpisodeReward=11\n",
      "Score 23 Total Steps 23\n",
      "EP151 EpisodeReward=12\n",
      "Score 27 Total Steps 27\n",
      "EP152 EpisodeReward=16\n",
      "Score 28 Total Steps 28\n",
      "EP153 EpisodeReward=17\n",
      "Score 12 Total Steps 12\n",
      "EP154 EpisodeReward=1\n",
      "Score 27 Total Steps 27\n",
      "EP155 EpisodeReward=16\n",
      "Score 28 Total Steps 28\n",
      "EP156 EpisodeReward=17\n",
      "Score 27 Total Steps 27\n",
      "EP157 EpisodeReward=16\n",
      "Score 28 Total Steps 28\n",
      "EP158 EpisodeReward=17\n",
      "Score 26 Total Steps 26\n",
      "EP159 EpisodeReward=15\n",
      "Score 22 Total Steps 22\n",
      "EP160 EpisodeReward=11\n",
      "Score 26 Total Steps 26\n",
      "EP161 EpisodeReward=15\n",
      "Score 26 Total Steps 26\n",
      "EP162 EpisodeReward=15\n",
      "Score 14 Total Steps 14\n",
      "EP163 EpisodeReward=3\n",
      "Score 30 Total Steps 30\n",
      "EP164 EpisodeReward=19\n",
      "Score 30 Total Steps 30\n",
      "EP165 EpisodeReward=19\n",
      "Score 31 Total Steps 31\n",
      "EP166 EpisodeReward=20\n",
      "Score 25 Total Steps 25\n",
      "EP167 EpisodeReward=14\n",
      "Score 25 Total Steps 25\n",
      "EP168 EpisodeReward=14\n",
      "Score 26 Total Steps 26\n",
      "EP169 EpisodeReward=15\n",
      "Score 25 Total Steps 25\n",
      "EP170 EpisodeReward=14\n",
      "Score 26 Total Steps 26\n",
      "EP171 EpisodeReward=15\n",
      "Score 24 Total Steps 24\n",
      "EP172 EpisodeReward=13\n",
      "Score 27 Total Steps 27\n",
      "EP173 EpisodeReward=16\n",
      "Score 25 Total Steps 25\n",
      "EP174 EpisodeReward=14\n",
      "Score 28 Total Steps 28\n",
      "EP175 EpisodeReward=17\n",
      "Score 28 Total Steps 28\n",
      "EP176 EpisodeReward=17\n",
      "Score 11 Total Steps 11\n",
      "EP177 EpisodeReward=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 24 Total Steps 24\n",
      "EP178 EpisodeReward=13\n",
      "Score 29 Total Steps 29\n",
      "EP179 EpisodeReward=18\n",
      "Score 26 Total Steps 26\n",
      "EP180 EpisodeReward=15\n",
      "Score 30 Total Steps 30\n",
      "EP181 EpisodeReward=19\n",
      "Score 23 Total Steps 23\n",
      "EP182 EpisodeReward=12\n",
      "Score 28 Total Steps 28\n",
      "EP183 EpisodeReward=17\n",
      "Score 27 Total Steps 27\n",
      "EP184 EpisodeReward=16\n",
      "Score 28 Total Steps 28\n",
      "EP185 EpisodeReward=17\n",
      "Score 29 Total Steps 28\n",
      "EP186 EpisodeReward=18\n",
      "Score 27 Total Steps 27\n",
      "EP187 EpisodeReward=16\n",
      "Score 21 Total Steps 21\n",
      "EP188 EpisodeReward=10\n",
      "Score 27 Total Steps 27\n",
      "EP189 EpisodeReward=16\n",
      "Score 22 Total Steps 22\n",
      "EP190 EpisodeReward=11\n",
      "Score 25 Total Steps 25\n",
      "EP191 EpisodeReward=14\n",
      "Score 27 Total Steps 27\n",
      "EP192 EpisodeReward=16\n",
      "Score 31 Total Steps 30\n",
      "EP193 EpisodeReward=20\n",
      "Score 28 Total Steps 28\n",
      "EP194 EpisodeReward=17\n",
      "Score 13 Total Steps 13\n",
      "EP195 EpisodeReward=2\n",
      "Score 23 Total Steps 23\n",
      "EP196 EpisodeReward=12\n",
      "Score 26 Total Steps 26\n",
      "EP197 EpisodeReward=15\n",
      "Score 26 Total Steps 26\n",
      "EP198 EpisodeReward=15\n",
      "Score 21 Total Steps 21\n",
      "EP199 EpisodeReward=10\n",
      "Score 28 Total Steps 28\n",
      "EP200 EpisodeReward=17\n",
      "Score 24 Total Steps 24\n",
      "EP201 EpisodeReward=13\n",
      "Score 24 Total Steps 24\n",
      "EP202 EpisodeReward=13\n",
      "Score 22 Total Steps 22\n",
      "EP203 EpisodeReward=11\n",
      "Score 28 Total Steps 28\n",
      "EP204 EpisodeReward=17\n",
      "Score 27 Total Steps 27\n",
      "EP205 EpisodeReward=16\n",
      "Score 26 Total Steps 26\n",
      "EP206 EpisodeReward=15\n",
      "Score 27 Total Steps 27\n",
      "EP207 EpisodeReward=16\n",
      "Score 13 Total Steps 13\n",
      "EP208 EpisodeReward=2\n",
      "Score 25 Total Steps 25\n",
      "EP209 EpisodeReward=14\n",
      "Score 27 Total Steps 27\n",
      "EP210 EpisodeReward=16\n",
      "Score 22 Total Steps 22\n",
      "EP211 EpisodeReward=11\n",
      "Score 28 Total Steps 28\n",
      "EP212 EpisodeReward=17\n",
      "Score 26 Total Steps 26\n",
      "EP213 EpisodeReward=15\n",
      "Score 24 Total Steps 24\n",
      "EP214 EpisodeReward=13\n",
      "Score 27 Total Steps 27\n",
      "EP215 EpisodeReward=16\n",
      "Score 12 Total Steps 12\n",
      "EP216 EpisodeReward=1\n",
      "Score 30 Total Steps 29\n",
      "EP217 EpisodeReward=19\n",
      "Score 15 Total Steps 15\n",
      "EP218 EpisodeReward=4\n",
      "Score 30 Total Steps 29\n",
      "EP219 EpisodeReward=19\n",
      "Score 29 Total Steps 29\n",
      "EP220 EpisodeReward=18\n",
      "Score 11 Total Steps 11\n",
      "EP221 EpisodeReward=0\n",
      "Score 26 Total Steps 26\n",
      "EP222 EpisodeReward=15\n",
      "Score 29 Total Steps 29\n",
      "EP223 EpisodeReward=18\n",
      "Score 18 Total Steps 18\n",
      "EP224 EpisodeReward=7\n",
      "Score 27 Total Steps 27\n",
      "EP225 EpisodeReward=16\n",
      "Score 23 Total Steps 23\n",
      "EP226 EpisodeReward=12\n",
      "Score 29 Total Steps 29\n",
      "EP227 EpisodeReward=18\n",
      "Score 26 Total Steps 26\n",
      "EP228 EpisodeReward=15\n",
      "Score 28 Total Steps 28\n",
      "EP229 EpisodeReward=17\n",
      "Score 28 Total Steps 28\n",
      "EP230 EpisodeReward=17\n",
      "Score 23 Total Steps 23\n",
      "EP231 EpisodeReward=12\n",
      "Score 26 Total Steps 26\n",
      "EP232 EpisodeReward=15\n",
      "Score 28 Total Steps 28\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c04e611c1d44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-c04e611c1d44>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtetris\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTertris\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-c04e611c1d44>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, max_episodes)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'EP{} EpisodeReward={}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-c04e611c1d44>\u001b[0m in \u001b[0;36mreplay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m             \u001b[0mnext_q_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m             \u001b[1;31m#print(states, actions, rewards, next_states, done,next_q_values )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnext_q_values\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-c04e611c1d44>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1627\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1629\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    860\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "class QNetwork(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.state_dim  = 216\n",
    "        self.action_dim = 40\n",
    "        self.epsilon = 1.\n",
    "        self.dense1 = tf.keras.layers.Dense(units=216, input_dim=216,activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=150, activation=tf.nn.relu)\n",
    "        self.dense3 = tf.keras.layers.Dense(units=40, activation=tf.nn.relu)\n",
    "        self.dense4 = tf.keras.layers.Dense(units=self.action_dim)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.model = self.create_model()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "    \n",
    "    def create_model(self):\n",
    "#         model = tf.keras.Sequential([\n",
    "#             Input((self.state_dim,)),\n",
    "#             Dense(32, activation='relu'),\n",
    "#             Dense(16, activation='relu'),\n",
    "#             Dense(self.action_dim)\n",
    "#         ])\n",
    "        model = tf.keras.models.Sequential()\n",
    "        model.add(self.dense1)\n",
    "        model.add(self.dense2)\n",
    "        model.add(self.dense3)\n",
    "        model.add(self.dense4)\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def predict(self, state):\n",
    "        return self.model.predict(state)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = np.reshape(state, [1, self.state_dim])\n",
    "        self.epsilon *= eps_decay\n",
    "        self.epsilon = max(self.epsilon, eps_min)\n",
    "        q_value = self.predict(state)[0]\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return random.randint(0, 39)\n",
    "        \n",
    "        \n",
    "        return np.argmax(q_value)\n",
    "    \n",
    "    def train(self, states, targets):\n",
    "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def put(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append([state, action, reward, next_state, done])\n",
    "    \n",
    "    def sample(self):\n",
    "        sample = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, done = map(np.asarray, zip(*sample))\n",
    "        states = np.array(states).reshape(batch_size, -1)\n",
    "        next_states = np.array(next_states).reshape(batch_size, -1)\n",
    "        return states, actions, rewards, next_states, done\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.state_dim = 216\n",
    "        self.action_dim = 40\n",
    "\n",
    "        self.model = QNetwork()\n",
    "        self.target_model = QNetwork()\n",
    "        self.target_update()\n",
    "\n",
    "        self.buffer = ReplayBuffer()\n",
    "\n",
    "    def target_update(self):\n",
    "        weights = self.model.model.get_weights()\n",
    "        self.target_model.model.set_weights(weights)\n",
    "    \n",
    "    def replay(self):\n",
    "        for _ in range(10):\n",
    "            states, actions, rewards, next_states, done = self.buffer.sample()\n",
    "            targets = self.target_model.predict(states)\n",
    "            next_q_values = self.target_model.predict(next_states).max(axis=1)\n",
    "            #print(states, actions, rewards, next_states, done,next_q_values )\n",
    "            targets[range(batch_size), actions] = rewards + (1-done) * next_q_values * gamma\n",
    "            self.model.train(states, targets)\n",
    "    \n",
    "    def train(self, max_episodes=1000):\n",
    "        for ep in range(max_episodes):\n",
    "            done, total_reward = False, 0\n",
    "            state = self.env.reset()\n",
    "            while not done:\n",
    "                action = self.model.get_action(state)\n",
    "                #print(action)\n",
    "                next_state, reward, done, info = self.env.step_action(action)\n",
    "                self.buffer.put(state, action, reward, next_state, done)\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "            print(\"Score\",self.env.score,\"Total Steps\",self.env.total_steps)\n",
    "            \n",
    "            \n",
    "            if self.buffer.size() >= batch_size:\n",
    "                self.replay()\n",
    "            self.target_update()\n",
    "            print('EP{} EpisodeReward={}'.format(ep, total_reward))\n",
    "            #wandb.log({'Reward': total_reward})\n",
    "\n",
    "\n",
    "def main():\n",
    "    env = tetris.Tertris(10,20)\n",
    "    agent = Agent(env)\n",
    "    agent.train(max_episodes=100000)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
